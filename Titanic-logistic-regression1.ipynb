{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2286,"sourceType":"datasetVersion","datasetId":1275}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic                                                        ","metadata":{}},{"cell_type":"markdown","source":"**Introduction:**\n\nWelcome to the Kaggle Notebook for the Titanic Survival Prediction! In this notebook, we'll explore and analyze the famous Titanic dataset, aiming to predict whether a passenger survived or not based on various features. The sinking of the Titanic is one of the most infamous maritime disasters in history, and this dataset provides a glimpse into the passengers' demographics and the factors that influenced their survival.","metadata":{}},{"cell_type":"markdown","source":"**Problem Statement:**\n\nThe primary goal of this analysis is to build a predictive model that can accurately predict whether a passenger survived or not. We'll leverage machine learning techniques, specifically logistic regression in this case, to achieve this. The insights gained from this analysis can offer valuable information about the factors that contributed to survival on the Titanic.\n\n","metadata":{}},{"cell_type":"markdown","source":"**Data Description:**\n\nThe dataset includes information about passengers on the Titanic, such as their age, gender, class, number of siblings/spouses aboard (SibSp), number of parents/children aboard (Parch), fare, and embarkation port. The target variable is 'Survived,' indicating whether a passenger survived (1) or not (0).\n\nThis dataset provides an excellent opportunity to explore the relationships between various features and survival outcomes.","metadata":{}},{"cell_type":"code","source":"#imported files\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data loaded\ndf = pd.read_csv('/kaggle/input/titanic/train_and_test2.csv')\ndf.head(n=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape of the data set\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking null values in the data set\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle missing values\ndf['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop unnecessary columns\ncolumns_to_drop = ['zero', 'zero.1', 'zero.2', 'zero.3', 'zero.4', 'zero.5', 'zero.6',\n                   'zero.7', 'zero.8', 'zero.9', 'zero.10', 'zero.11', 'zero.12',\n                   'zero.13', 'zero.14', 'zero.15', 'zero.16', 'zero.17', 'zero.18']\ndf = df.drop(columns=columns_to_drop)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rename the target variable\ndf.rename(columns={'2urvived': 'Survived'}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select features and target variable\nX = df[['Pclass', 'Sex', 'Age', 'sibsp', 'Parch', 'Fare', 'Embarked']]\ny = df['Survived']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Convert categorical variables into numerical using one-hot encoding\nX = pd.get_dummies(X, columns=['Sex', 'Embarked'], drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature scaling\nscaler = StandardScaler()\nX[['Age', 'Fare']] = scaler.fit_transform(X[['Age', 'Fare']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create and train the logistic regression model\nLR = LogisticRegression()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the test set\ny_pred = LR.predict(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model performance\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred) * 100\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the results\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"\\nAccuracy: {:.2f}%\".format(accuracy))\nprint(\"Precision: {:.2f}\".format(precision))\nprint(\"Recall: {:.2f}\".format(recall))\nprint(\"F1 Score: {:.2f}\".format(f1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion:**\n\nIn summary, the logistic regression model achieved an accuracy of 76.72% in predicting Titanic passenger survival. While the model demonstrated moderate precision (64%) in correctly identifying survivors, the recall (37%) indicates room for improvement in capturing all actual survivors. The overall F1 score is 0.47, suggesting a balanced performance between precision and recall.","metadata":{}}]}